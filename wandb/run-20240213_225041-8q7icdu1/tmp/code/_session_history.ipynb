{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccd90912",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install avalanche-lib==0.4\n",
    "!pip install einops\n",
    "!pip install warmup-scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e30741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, channels, hidden_dim):\n",
    "        super().__init__()\n",
    "        # Enhanced CNN architecture\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # First convolution block\n",
    "            nn.Conv2d(channels, hidden_dim//2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim//2, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_layers(x)\n",
    "\n",
    "class ResNet18FeatureExtractor(nn.Module):\n",
    "    def __init__(self, pretrained=False):\n",
    "        super().__init__()\n",
    "        # Load a pre-trained ResNet-18 model\n",
    "        self.resnet18 = models.resnet18(pretrained=pretrained)\n",
    "\n",
    "        # Remove the fully connected layer\n",
    "        # Alternatively, you can use AdaptiveAvgPool2d to control the output size\n",
    "        self.resnet18 = nn.Sequential(*list(self.resnet18.children())[:-2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet18(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.downsample = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class SmallResNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # BasicBlock with downsampling to reduce spatial dimensions\n",
    "        self.layer1 = BasicBlock(16, 32, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
    "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "\n",
    "        return self.norm(x)\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', img_channels = 1, feature_channels = 1, dim_head = 64, dropout = 0., emb_dropout = 0., n_examples = 1, device = 'cpu'):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = feature_channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "        ### positional embedding options: 1. learned 2. sinusoidal ###\n",
    "        # self.pos_embedding = nn.Parameter(torch.randn(1, (num_patches*2 + 3)*(examples), dim))\n",
    "        if n_examples == 0:\n",
    "            self.pos_embedding = self.sinusoidal_embeddings(num_patches * n_examples + 1, dim).to(device)\n",
    "        else:\n",
    "            self.pos_embedding = self.sinusoidal_embeddings((num_patches*2  + 4) * n_examples, dim).to(device)\n",
    "\n",
    "        self.comma_token = nn.Parameter(torch.randn(1, 1, dim, device= device)) # Token for ','\n",
    "        self.arrow_token = nn.Parameter(torch.randn(1, 1, dim, device= device))  # Token for '->'\n",
    "        self.pipe_token = nn.Parameter(torch.randn(1, 1, dim, device= device))   # Token for '|'\n",
    "        self.clf_token = nn.Parameter(torch.randn(1, 1, dim, device= device))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "        self.cnn_feature_extractor = CNNFeatureExtractor(img_channels, feature_channels)\n",
    "        self.resnet_feature_extractor =  SmallResNetFeatureExtractor()\n",
    "\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Linear(dim, num_classes)\n",
    "        self.n_examples = n_examples\n",
    "        self.dim = dim\n",
    "\n",
    "        self.saved_imgs = None\n",
    "        self.saved_labels = None\n",
    "        self.device = device\n",
    "\n",
    "    def sinusoidal_embeddings(self, n_pos, dim):\n",
    "        position = torch.arange(0, n_pos, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))\n",
    "        sinusoidal_emb = torch.zeros(n_pos, dim)\n",
    "        sinusoidal_emb[:, 0::2] = torch.sin(position * div_term)\n",
    "        sinusoidal_emb[:, 1::2] = torch.cos(position * div_term)\n",
    "        return sinusoidal_emb.unsqueeze(0)\n",
    "\n",
    "    def forward(self, mbatch):\n",
    "\n",
    "        ## mbatch contains labels\n",
    "        if mbatch[0].dim() == 4:\n",
    "            imgs = mbatch[0]\n",
    "            labels = mbatch[1]\n",
    "            n_examples = self.n_examples\n",
    "        ## mbatch contains images only\n",
    "        else:\n",
    "            imgs = mbatch\n",
    "            n_examples = 0\n",
    "\n",
    "\n",
    "        batch_size, _, _, _ = imgs.shape\n",
    "\n",
    "        seq = []\n",
    "\n",
    "         ## training and testing without exemples, the pipe token can serve as the cls token used for ViT classification\n",
    "        if n_examples == 0:\n",
    "            seq.append(repeat(self.pipe_token, '1 1 d -> b 1 d', b=batch_size))\n",
    "            seq.append(self.to_patch_embedding(imgs))\n",
    "        else:\n",
    "            if self.training:\n",
    "\n",
    "                if self.saved_imgs == None:\n",
    "                    self.saved_imgs = imgs[:n_examples]\n",
    "                    self.saved_labels = labels[:n_examples]\n",
    "\n",
    "                for i in range(n_examples):\n",
    "                    # Shift images and labels by 1 index\n",
    "                    # shifting and append aligns instances with other instances in the same batch, while these instances serve as the examples\n",
    "                    shifted_imgs = torch.roll(imgs, shifts=-i-1, dims=0)\n",
    "                    shifted_labels = torch.roll(labels, shifts=-i-1, dims=0)\n",
    "\n",
    "                    # # Convert labels to one-hot with length equal to embedding dimension (self.dim)\n",
    "                    shifted_labels = nn.functional.one_hot(shifted_labels, num_classes=self.dim).float()\n",
    "\n",
    "                    # append example (image, delimitor, label)\n",
    "                    seq.append(self.to_patch_embedding(shifted_imgs))\n",
    "                    seq.append(repeat(self.arrow_token, '1 1 d -> b 1 d', b=batch_size))\n",
    "                    seq.append(shifted_labels.unsqueeze(1))\n",
    "\n",
    "                    # append target (image)\n",
    "                    seq.append(repeat(self.pipe_token, '1 1 d -> b 1 d', b=batch_size))\n",
    "                    seq.append(self.to_patch_embedding(imgs))\n",
    "                    seq.append(repeat(self.clf_token, '1 1 d -> b 1 d', b=batch_size))\n",
    "\n",
    "            else:\n",
    "                if self.saved_imgs == None:\n",
    "                    self.saved_imgs = imgs[:n_examples]\n",
    "                    self.saved_labels = labels[:n_examples]\n",
    "                for i in range(n_examples):\n",
    "                    # Shift images and labels by 1 index\n",
    "                    # shifting and append aligns instances with other instances in the same batch, while these instances serve as the examples\n",
    "                    saved_img = self.saved_imgs[i].repeat(batch_size,1,1,1)\n",
    "                    saved_label = nn.functional.one_hot(self.saved_labels[i].repeat(batch_size,), num_classes=self.dim).float()\n",
    "\n",
    "                    seq.append(self.to_patch_embedding(saved_img))\n",
    "                    seq.append(repeat(self.arrow_token, '1 1 d -> b 1 d', b=batch_size))\n",
    "                    seq.append(saved_label.unsqueeze(1))\n",
    "                    # append target (image)\n",
    "                    seq.append(repeat(self.pipe_token, '1 1 d -> b 1 d', b=batch_size))\n",
    "                    seq.append(self.to_patch_embedding(imgs))\n",
    "                    seq.append(repeat(self.clf_token, '1 1 d -> b 1 d', b=batch_size))\n",
    "\n",
    "        seq = torch.cat(seq, dim=1)\n",
    "\n",
    "        # Add positional embeddings, ensure they match the sequence length\n",
    "        x = seq + self.pos_embedding[:, :seq.size(1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, -1,:]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        x = self.mlp_head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b295caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import avalanche\n",
    "from avalanche.core import SupervisedPlugin\n",
    "class FewShotMetaLearningPlugin(SupervisedPlugin):\n",
    "    # ... other methods ...\n",
    "\n",
    "    def before_forward(self, strategy: \"SupervisedTemplate\", **kwargs):\n",
    "        \"\"\"\n",
    "        Hook called before the forward pass of the model during training.\n",
    "        Our model takes additional label information, we provide it as a (inputs, labels) tuple\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = strategy.mbatch[0]  # Inputs (features)\n",
    "        labels = strategy.mbatch[1]  # Labels (targets)\n",
    "\n",
    "        # Package inputs and labels in the format expected by the model\n",
    "        packaged_input = (inputs, labels)\n",
    "\n",
    "        # Set packaged_input, check the api document for other available controls. mbatch is sent as the only argument taken in the forward function\n",
    "        strategy.mbatch[0] = packaged_input\n",
    "\n",
    "    def before_eval_forward(self, strategy: \"SupervisedTemplate\", **kwargs):\n",
    "        \"\"\"\n",
    "        Hook called before the forward pass of the model during testing.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        inputs = strategy.mbatch[0]  # Inputs (features)\n",
    "        labels = strategy.mbatch[1]  # Labels (targets)\n",
    "\n",
    "        # Package inputs and labels in the format expected by the model\n",
    "        packaged_input = (inputs, labels)\n",
    "\n",
    "        # Set packaged_input\n",
    "        strategy.mbatch[0] = packaged_input\n",
    "        strategy.mbatch[0] = packaged_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16b8c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torchvision.transforms as transforms\n",
    "from avalanche.benchmarks.classic import SplitMNIST, SplitCIFAR100,SplitTinyImageNet, CORe50\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics, \\\n",
    "    loss_metrics, timing_metrics, cpu_usage_metrics, confusion_matrix_metrics, disk_usage_metrics\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger, WandBLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.training.supervised import Naive\n",
    "from avalanche.training.templates import SupervisedTemplate\n",
    "from avalanche.training.plugins import ReplayPlugin, EWCPlugin, LRSchedulerPlugin\n",
    "from avalanche.training.plugins import BiCPlugin\n",
    "\n",
    "\n",
    "# there's a bug in wandb logging of the last iteration, we have to mannually finish it\n",
    "import wandb\n",
    "import torch.optim as optim\n",
    "import warmup_scheduler\n",
    "# scenario = SplitMNIST(n_experiences=5, seed = 1234)\n",
    "\n",
    "\n",
    "\n",
    "# scenario = SplitCIFAR100(n_experiences=10, seed = 1234)\n",
    "# scenario = SplitTinyImageNet(n_experiences=10, seed = 1234)\n",
    "scenario = CORe50(mini=True)\n",
    "device = 'cuda:0'\n",
    "# MODEL CREATION\n",
    "\n",
    "# model = ViT(\n",
    "#     image_size = 28,\n",
    "#     patch_size = 7,\n",
    "#     num_classes = 10,\n",
    "#     dim = 512,\n",
    "#     depth = 3,\n",
    "#     heads = 8,\n",
    "#     mlp_dim = 256,\n",
    "#     dropout = 0.1,\n",
    "#     emb_dropout = 0.1,\n",
    "#     img_channels = 1,\n",
    "#     feature_channels = 1,\n",
    "#     dim_head = 64\n",
    "# )\n",
    "\n",
    "model = ViT(\n",
    "    image_size = 32,\n",
    "    patch_size = 4,\n",
    "    num_classes = 100,\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 384,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1,\n",
    "    img_channels = 3,\n",
    "    feature_channels = 3,\n",
    "    dim_head = 32,\n",
    "    device=device\n",
    ")\n",
    "# DEFINE THE EVALUATION PLUGIN and LOGGERS\n",
    "# The evaluation plugin manages the metrics computation.\n",
    "# It takes as argument a list of metrics, collectes their results and returns\n",
    "# them to the strategy it is attached to.\n",
    "\n",
    "loggers = []\n",
    "\n",
    "# log to Tensorboard\n",
    "# log to text file\n",
    "loggers.append(TextLogger(open('log.txt', 'a')))\n",
    "\n",
    "# print to stdout\n",
    "loggers.append(InteractiveLogger())\n",
    "\n",
    "# W&B logger - comment this if you don't have a W&B account\n",
    "loggers.append(WandBLogger(project_name=\"avalanche\", run_name=\"CIFAR100-input-nolabel\"))\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(epoch=True, epoch_running=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    confusion_matrix_metrics(num_classes=scenario.n_classes, save_image=False,\n",
    "                             stream=True),\n",
    "    loggers=loggers\n",
    ")\n",
    "\n",
    "\n",
    "# strategy plugign\n",
    "replay = ReplayPlugin(mem_size=1000)\n",
    "fsml = FewShotMetaLearningPlugin()\n",
    "ewc = EWCPlugin(ewc_lambda=1)\n",
    "bic = BiCPlugin(mem_size=1000)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3, weight_decay = 0.00005)\n",
    "# Assuming your optimizer is already defined\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 400, eta_min=1e-4)\n",
    "lr_scheduler_plugin = LRSchedulerPlugin(scheduler)\n",
    "\n",
    "# CREATE THE STRATEGY INSTANCE\n",
    "cl_strategy = SupervisedTemplate(\n",
    "    model, optimizer,\n",
    "    CrossEntropyLoss(), train_mb_size=128, train_epochs=10, eval_mb_size=128, device=device,\n",
    "    evaluator=eval_plugin,plugins=[fsml, replay, lr_scheduler_plugin])\n",
    "\n",
    "\n",
    "\n",
    "# TRAINING LOOP\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "for i,experience in enumerate(scenario.train_stream[:]):\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    # train returns a dictionary which contains all the metric values\n",
    "    if i == 0:\n",
    "        cl_strategy.train_epochs = 10\n",
    "    else:\n",
    "        cl_strategy.train_epochs = 10\n",
    "\n",
    "    res = cl_strategy.train(experience, eval_streams=[scenario.test_stream[:i+1]])\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the whole test set')\n",
    "    # test also returns a dictionary which contains all the metric values\n",
    "    results.append(cl_strategy.eval(scenario.test_stream[:i+1]))\n",
    "\n",
    "# there's a bug in wandb logging of the last iteration, we have to mannually finish it\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5b84c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torchvision.transforms as transforms\n",
    "from avalanche.benchmarks.classic import SplitMNIST, SplitCIFAR100,SplitTinyImageNet, CORe50\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics, \\\n",
    "    loss_metrics, timing_metrics, cpu_usage_metrics, confusion_matrix_metrics, disk_usage_metrics\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger, WandBLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.training.supervised import Naive\n",
    "from avalanche.training.templates import SupervisedTemplate\n",
    "from avalanche.training.plugins import ReplayPlugin, EWCPlugin, LRSchedulerPlugin\n",
    "from avalanche.training.plugins import BiCPlugin\n",
    "\n",
    "\n",
    "# there's a bug in wandb logging of the last iteration, we have to mannually finish it\n",
    "import wandb\n",
    "import torch.optim as optim\n",
    "import warmup_scheduler\n",
    "# scenario = SplitMNIST(n_experiences=5, seed = 1234)\n",
    "\n",
    "\n",
    "\n",
    "# scenario = SplitCIFAR100(n_experiences=10, seed = 1234)\n",
    "# scenario = SplitTinyImageNet(n_experiences=10, seed = 1234)\n",
    "scenario = CORe50(mini=True)\n",
    "device = 'cuda:0'\n",
    "# MODEL CREATION\n",
    "\n",
    "# model = ViT(\n",
    "#     image_size = 28,\n",
    "#     patch_size = 7,\n",
    "#     num_classes = 10,\n",
    "#     dim = 512,\n",
    "#     depth = 3,\n",
    "#     heads = 8,\n",
    "#     mlp_dim = 256,\n",
    "#     dropout = 0.1,\n",
    "#     emb_dropout = 0.1,\n",
    "#     img_channels = 1,\n",
    "#     feature_channels = 1,\n",
    "#     dim_head = 64\n",
    "# )\n",
    "\n",
    "model = ViT(\n",
    "    image_size = 32,\n",
    "    patch_size = 4,\n",
    "    num_classes = 100,\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 384,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1,\n",
    "    img_channels = 3,\n",
    "    feature_channels = 3,\n",
    "    dim_head = 32,\n",
    "    device=device\n",
    ")\n",
    "# DEFINE THE EVALUATION PLUGIN and LOGGERS\n",
    "# The evaluation plugin manages the metrics computation.\n",
    "# It takes as argument a list of metrics, collectes their results and returns\n",
    "# them to the strategy it is attached to.\n",
    "\n",
    "loggers = []\n",
    "\n",
    "# log to Tensorboard\n",
    "# log to text file\n",
    "loggers.append(TextLogger(open('log.txt', 'a')))\n",
    "\n",
    "# print to stdout\n",
    "loggers.append(InteractiveLogger())\n",
    "\n",
    "# W&B logger - comment this if you don't have a W&B account\n",
    "loggers.append(WandBLogger(project_name=\"avalanche\", run_name=\"CIFAR100-input-nolabel\"))\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(epoch=True, epoch_running=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    confusion_matrix_metrics(num_classes=scenario.n_classes, save_image=False,\n",
    "                             stream=True),\n",
    "    loggers=loggers\n",
    ")\n",
    "\n",
    "\n",
    "# strategy plugign\n",
    "replay = ReplayPlugin(mem_size=1000)\n",
    "fsml = FewShotMetaLearningPlugin()\n",
    "ewc = EWCPlugin(ewc_lambda=1)\n",
    "bic = BiCPlugin(mem_size=1000)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3, weight_decay = 0.00005)\n",
    "# Assuming your optimizer is already defined\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 10, eta_min=1e-4)\n",
    "lr_scheduler_plugin = LRSchedulerPlugin(scheduler)\n",
    "\n",
    "# CREATE THE STRATEGY INSTANCE\n",
    "cl_strategy = SupervisedTemplate(\n",
    "    model, optimizer,\n",
    "    CrossEntropyLoss(), train_mb_size=128, train_epochs=10, eval_mb_size=128, device=device,\n",
    "    evaluator=eval_plugin,plugins=[fsml, replay, lr_scheduler_plugin])\n",
    "\n",
    "\n",
    "\n",
    "# TRAINING LOOP\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "for i,experience in enumerate(scenario.train_stream[:]):\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    # train returns a dictionary which contains all the metric values\n",
    "    res = cl_strategy.train(experience, eval_streams=[scenario.test_stream[:i+1]])\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the whole test set')\n",
    "    # test also returns a dictionary which contains all the metric values\n",
    "    results.append(cl_strategy.eval(scenario.test_stream[:i+1]))\n",
    "\n",
    "# there's a bug in wandb logging of the last iteration, we have to mannually finish it\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6212cb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torchvision.transforms as transforms\n",
    "from avalanche.benchmarks.classic import SplitMNIST, SplitCIFAR100,SplitTinyImageNet, CORe50\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics, \\\n",
    "    loss_metrics, timing_metrics, cpu_usage_metrics, confusion_matrix_metrics, disk_usage_metrics\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger, WandBLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.training.supervised import Naive\n",
    "from avalanche.training.templates import SupervisedTemplate\n",
    "from avalanche.training.plugins import ReplayPlugin, EWCPlugin, LRSchedulerPlugin\n",
    "from avalanche.training.plugins import BiCPlugin\n",
    "\n",
    "\n",
    "# there's a bug in wandb logging of the last iteration, we have to mannually finish it\n",
    "import wandb\n",
    "import torch.optim as optim\n",
    "import warmup_scheduler\n",
    "# scenario = SplitMNIST(n_experiences=5, seed = 1234)\n",
    "\n",
    "\n",
    "\n",
    "# scenario = SplitCIFAR100(n_experiences=10, seed = 1234)\n",
    "# scenario = SplitTinyImageNet(n_experiences=10, seed = 1234)\n",
    "scenario = CORe50(mini=True)\n",
    "device = 'cuda:0'\n",
    "# MODEL CREATION\n",
    "\n",
    "# model = ViT(\n",
    "#     image_size = 28,\n",
    "#     patch_size = 7,\n",
    "#     num_classes = 10,\n",
    "#     dim = 512,\n",
    "#     depth = 3,\n",
    "#     heads = 8,\n",
    "#     mlp_dim = 256,\n",
    "#     dropout = 0.1,\n",
    "#     emb_dropout = 0.1,\n",
    "#     img_channels = 1,\n",
    "#     feature_channels = 1,\n",
    "#     dim_head = 64\n",
    "# )\n",
    "\n",
    "model = ViT(\n",
    "    image_size = 32,\n",
    "    patch_size = 4,\n",
    "    num_classes = 100,\n",
    "    dim = 384,\n",
    "    depth = 6,\n",
    "    heads = 12,\n",
    "    mlp_dim = 384,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1,\n",
    "    img_channels = 3,\n",
    "    feature_channels = 3,\n",
    "    dim_head = 32,\n",
    "    device=device\n",
    ")\n",
    "# DEFINE THE EVALUATION PLUGIN and LOGGERS\n",
    "# The evaluation plugin manages the metrics computation.\n",
    "# It takes as argument a list of metrics, collectes their results and returns\n",
    "# them to the strategy it is attached to.\n",
    "\n",
    "loggers = []\n",
    "\n",
    "# log to Tensorboard\n",
    "# log to text file\n",
    "loggers.append(TextLogger(open('log.txt', 'a')))\n",
    "\n",
    "# print to stdout\n",
    "loggers.append(InteractiveLogger())\n",
    "\n",
    "# W&B logger - comment this if you don't have a W&B account\n",
    "loggers.append(WandBLogger(project_name=\"avalanche\", run_name=\"CIFAR100-input-nolabel\"))\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(epoch=True, epoch_running=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    confusion_matrix_metrics(num_classes=scenario.n_classes, save_image=False,\n",
    "                             stream=True),\n",
    "    loggers=loggers\n",
    ")\n",
    "\n",
    "\n",
    "# strategy plugign\n",
    "replay = ReplayPlugin(mem_size=1000)\n",
    "fsml = FewShotMetaLearningPlugin()\n",
    "ewc = EWCPlugin(ewc_lambda=1)\n",
    "bic = BiCPlugin(mem_size=1000)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3, weight_decay = 0.00005)\n",
    "# Assuming your optimizer is already defined\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 10, eta_min=1e-4)\n",
    "lr_scheduler_plugin = LRSchedulerPlugin(scheduler)\n",
    "\n",
    "# CREATE THE STRATEGY INSTANCE\n",
    "cl_strategy = SupervisedTemplate(\n",
    "    model, optimizer,\n",
    "    CrossEntropyLoss(), train_mb_size=128, train_epochs=10, eval_mb_size=128, device=device,\n",
    "    evaluator=eval_plugin,plugins=[fsml, replay, lr_scheduler_plugin])\n",
    "\n",
    "\n",
    "\n",
    "# TRAINING LOOP\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "for i,experience in enumerate(scenario.train_stream[:]):\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    # train returns a dictionary which contains all the metric values\n",
    "    res = cl_strategy.train(experience, eval_streams=[scenario.test_stream[:i+1]])\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the whole test set')\n",
    "    # test also returns a dictionary which contains all the metric values\n",
    "    results.append(cl_strategy.eval(scenario.test_stream[:i+1]))\n",
    "\n",
    "# there's a bug in wandb logging of the last iteration, we have to mannually finish it\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a34452c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torchvision.transforms as transforms\n",
    "from avalanche.benchmarks.classic import SplitMNIST, SplitCIFAR100,SplitTinyImageNet, CORe50\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics, \\\n",
    "    loss_metrics, timing_metrics, cpu_usage_metrics, confusion_matrix_metrics, disk_usage_metrics\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger, WandBLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.training.supervised import Naive\n",
    "from avalanche.training.templates import SupervisedTemplate\n",
    "from avalanche.training.plugins import ReplayPlugin, EWCPlugin, LRSchedulerPlugin\n",
    "from avalanche.training.plugins import BiCPlugin\n",
    "\n",
    "\n",
    "# there's a bug in wandb logging of the last iteration, we have to mannually finish it\n",
    "import wandb\n",
    "import torch.optim as optim\n",
    "import warmup_scheduler\n",
    "# scenario = SplitMNIST(n_experiences=5, seed = 1234)\n",
    "\n",
    "\n",
    "\n",
    "# scenario = SplitCIFAR100(n_experiences=10, seed = 1234)\n",
    "# scenario = SplitTinyImageNet(n_experiences=10, seed = 1234)\n",
    "scenario = CORe50(mini=True)\n",
    "device = 'cuda:0'\n",
    "# MODEL CREATION\n",
    "\n",
    "# model = ViT(\n",
    "#     image_size = 28,\n",
    "#     patch_size = 7,\n",
    "#     num_classes = 10,\n",
    "#     dim = 512,\n",
    "#     depth = 3,\n",
    "#     heads = 8,\n",
    "#     mlp_dim = 256,\n",
    "#     dropout = 0.1,\n",
    "#     emb_dropout = 0.1,\n",
    "#     img_channels = 1,\n",
    "#     feature_channels = 1,\n",
    "#     dim_head = 64\n",
    "# )\n",
    "\n",
    "model = ViT(\n",
    "    image_size = 32,\n",
    "    patch_size = 4,\n",
    "    num_classes = 100,\n",
    "    dim = 384,\n",
    "    depth = 6,\n",
    "    heads = 12,\n",
    "    mlp_dim = 384,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1,\n",
    "    img_channels = 3,\n",
    "    feature_channels = 3,\n",
    "    dim_head = 32,\n",
    "    device=device\n",
    ")\n",
    "# DEFINE THE EVALUATION PLUGIN and LOGGERS\n",
    "# The evaluation plugin manages the metrics computation.\n",
    "# It takes as argument a list of metrics, collectes their results and returns\n",
    "# them to the strategy it is attached to.\n",
    "\n",
    "loggers = []\n",
    "\n",
    "# log to Tensorboard\n",
    "# log to text file\n",
    "loggers.append(TextLogger(open('log.txt', 'a')))\n",
    "\n",
    "# print to stdout\n",
    "loggers.append(InteractiveLogger())\n",
    "\n",
    "# W&B logger - comment this if you don't have a W&B account\n",
    "loggers.append(WandBLogger(project_name=\"avalanche\", run_name=\"CIFAR100-input-nolabel\"))\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(epoch=True, epoch_running=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    confusion_matrix_metrics(num_classes=scenario.n_classes, save_image=False,\n",
    "                             stream=True),\n",
    "    loggers=loggers\n",
    ")\n",
    "\n",
    "\n",
    "# strategy plugign\n",
    "replay = ReplayPlugin(mem_size=1000)\n",
    "fsml = FewShotMetaLearningPlugin()\n",
    "ewc = EWCPlugin(ewc_lambda=1)\n",
    "bic = BiCPlugin(mem_size=1000)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3, weight_decay = 0.00005)\n",
    "# Assuming your optimizer is already defined\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 10, eta_min=1e-4)\n",
    "lr_scheduler_plugin = LRSchedulerPlugin(scheduler)\n",
    "\n",
    "# CREATE THE STRATEGY INSTANCE\n",
    "cl_strategy = SupervisedTemplate(\n",
    "    model, optimizer,\n",
    "    CrossEntropyLoss(), train_mb_size=128, train_epochs=10, eval_mb_size=128, device=device,\n",
    "    evaluator=eval_plugin,plugins=[fsml, replay, lr_scheduler_plugin], eval_every=1)\n",
    "\n",
    "\n",
    "\n",
    "# TRAINING LOOP\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "for i,experience in enumerate(scenario.train_stream[:]):\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    # train returns a dictionary which contains all the metric values\n",
    "    res = cl_strategy.train(experience, eval_streams=[scenario.test_stream[:i+1]])\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the whole test set')\n",
    "    # test also returns a dictionary which contains all the metric values\n",
    "    results.append(cl_strategy.eval(scenario.test_stream[:i+1]))\n",
    "\n",
    "# there's a bug in wandb logging of the last iteration, we have to mannually finish it\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf8b3395",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, channels, hidden_dim):\n",
    "        super().__init__()\n",
    "        # Enhanced CNN architecture\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # First convolution block\n",
    "            nn.Conv2d(channels, hidden_dim//2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim//2, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_layers(x)\n",
    "\n",
    "class ResNet18FeatureExtractor(nn.Module):\n",
    "    def __init__(self, pretrained=False):\n",
    "        super().__init__()\n",
    "        # Load a pre-trained ResNet-18 model\n",
    "        self.resnet18 = models.resnet18(pretrained=pretrained)\n",
    "\n",
    "        # Remove the fully connected layer\n",
    "        # Alternatively, you can use AdaptiveAvgPool2d to control the output size\n",
    "        self.resnet18 = nn.Sequential(*list(self.resnet18.children())[:-2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet18(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.downsample = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class SmallResNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # BasicBlock with downsampling to reduce spatial dimensions\n",
    "        self.layer1 = BasicBlock(16, 32, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
    "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "\n",
    "        return self.norm(x)\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', img_channels = 1, feature_channels = 1, dim_head = 64, dropout = 0., emb_dropout = 0., n_examples = 1, device = 'cpu'):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = feature_channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "        ### positional embedding options: 1. learned 2. sinusoidal ###\n",
    "        # self.pos_embedding = nn.Parameter(torch.randn(1, (num_patches*2 + 3)*(examples), dim))\n",
    "        if n_examples == 0:\n",
    "            self.pos_embedding = self.sinusoidal_embeddings(num_patches * n_examples + 1, dim).to(device)\n",
    "        else:\n",
    "            self.pos_embedding = self.sinusoidal_embeddings((num_patches*2  + 4) * n_examples, dim).to(device)\n",
    "\n",
    "        self.comma_token = nn.Parameter(torch.randn(1, 1, dim, device= device)) # Token for ','\n",
    "        self.arrow_token = nn.Parameter(torch.randn(1, 1, dim, device= device))  # Token for '->'\n",
    "        self.pipe_token = nn.Parameter(torch.randn(1, 1, dim, device= device))   # Token for '|'\n",
    "        self.clf_token = nn.Parameter(torch.randn(1, 1, dim, device= device))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "        self.cnn_feature_extractor = CNNFeatureExtractor(img_channels, feature_channels)\n",
    "        self.resnet_feature_extractor =  SmallResNetFeatureExtractor()\n",
    "\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Linear(dim, num_classes)\n",
    "        self.n_examples = n_examples\n",
    "        self.dim = dim\n",
    "\n",
    "        self.saved_imgs = None\n",
    "        self.saved_labels = None\n",
    "        self.device = device\n",
    "\n",
    "    def sinusoidal_embeddings(self, n_pos, dim):\n",
    "        position = torch.arange(0, n_pos, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))\n",
    "        sinusoidal_emb = torch.zeros(n_pos, dim)\n",
    "        sinusoidal_emb[:, 0::2] = torch.sin(position * div_term)\n",
    "        sinusoidal_emb[:, 1::2] = torch.cos(position * div_term)\n",
    "        return sinusoidal_emb.unsqueeze(0)\n",
    "\n",
    "    def forward(self, mbatch):\n",
    "\n",
    "        ## mbatch contains labels\n",
    "        if mbatch[0].dim() == 4:\n",
    "            imgs = mbatch[0]\n",
    "            labels = mbatch[1]\n",
    "            n_examples = self.n_examples\n",
    "        ## mbatch contains images only\n",
    "        else:\n",
    "            imgs = mbatch\n",
    "            n_examples = 0\n",
    "\n",
    "\n",
    "        batch_size, _, _, _ = imgs.shape\n",
    "\n",
    "        seq = []\n",
    "\n",
    "         ## training and testing without exemples, the pipe token can serve as the cls token used for ViT classification\n",
    "        if n_examples == 0:\n",
    "            seq.append(repeat(self.pipe_token, '1 1 d -> b 1 d', b=batch_size))\n",
    "            seq.append(self.to_patch_embedding(imgs))\n",
    "        else:\n",
    "            if self.training:\n",
    "\n",
    "                if self.saved_imgs == None:\n",
    "                    self.saved_imgs = imgs[:n_examples]\n",
    "                    self.saved_labels = labels[:n_examples]\n",
    "\n",
    "                for i in range(n_examples):\n",
    "                    # Shift images and labels by 1 index\n",
    "                    # shifting and append aligns instances with other instances in the same batch, while these instances serve as the examples\n",
    "                    shifted_imgs = torch.roll(imgs, shifts=-i-1, dims=0)\n",
    "                    shifted_labels = torch.roll(labels, shifts=-i-1, dims=0)\n",
    "\n",
    "                    # # Convert labels to one-hot with length equal to embedding dimension (self.dim)\n",
    "                    shifted_labels = nn.functional.one_hot(shifted_labels, num_classes=self.dim).float()\n",
    "                    # append example (image, delimitor, label)\n",
    "                    seq.append(self.to_patch_embedding(shifted_imgs))\n",
    "                    seq.append(repeat(self.arrow_token, '1 1 d -> b 1 d', b=batch_size))\n",
    "                    seq.append(shifted_labels.unsqueeze(1))\n",
    "\n",
    "                    # append target (image)\n",
    "                    seq.append(repeat(self.pipe_token, '1 1 d -> b 1 d', b=batch_size))\n",
    "                    seq.append(self.to_patch_embedding(imgs))\n",
    "                    seq.append(repeat(self.clf_token, '1 1 d -> b 1 d', b=batch_size))\n",
    "\n",
    "            else:\n",
    "                if self.saved_imgs == None:\n",
    "                    self.saved_imgs = imgs[:n_examples]\n",
    "                    self.saved_labels = labels[:n_examples]\n",
    "                for i in range(n_examples):\n",
    "                    # Shift images and labels by 1 index\n",
    "                    # shifting and append aligns instances with other instances in the same batch, while these instances serve as the examples\n",
    "                    saved_img = self.saved_imgs[i].repeat(batch_size,1,1,1)\n",
    "                    saved_label = nn.functional.one_hot(self.saved_labels[i].repeat(batch_size,), num_classes=self.dim).float()\n",
    "\n",
    "                    seq.append(self.to_patch_embedding(saved_img))\n",
    "                    seq.append(repeat(self.arrow_token, '1 1 d -> b 1 d', b=batch_size))\n",
    "                    seq.append(saved_label.unsqueeze(1))\n",
    "                    # append target (image)\n",
    "                    seq.append(repeat(self.pipe_token, '1 1 d -> b 1 d', b=batch_size))\n",
    "                    seq.append(self.to_patch_embedding(imgs))\n",
    "                    seq.append(repeat(self.clf_token, '1 1 d -> b 1 d', b=batch_size))\n",
    "\n",
    "        seq = torch.cat(seq, dim=1)\n",
    "\n",
    "        # Add positional embeddings, ensure they match the sequence length\n",
    "        x = seq + self.pos_embedding[:, :seq.size(1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, -1]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        x = self.mlp_head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d4f6c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import avalanche\n",
    "from avalanche.core import SupervisedPlugin\n",
    "class FewShotMetaLearningPlugin(SupervisedPlugin):\n",
    "    # ... other methods ...\n",
    "\n",
    "    def before_forward(self, strategy: \"SupervisedTemplate\", **kwargs):\n",
    "        \"\"\"\n",
    "        Hook called before the forward pass of the model during training.\n",
    "        Our model takes additional label information, we provide it as a (inputs, labels) tuple\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = strategy.mbatch[0]  # Inputs (features)\n",
    "        labels = strategy.mbatch[1]  # Labels (targets)\n",
    "\n",
    "        # Package inputs and labels in the format expected by the model\n",
    "        packaged_input = (inputs, labels)\n",
    "\n",
    "        # Set packaged_input, check the api document for other available controls. mbatch is sent as the only argument taken in the forward function\n",
    "        strategy.mbatch[0] = packaged_input\n",
    "\n",
    "    def before_eval_forward(self, strategy: \"SupervisedTemplate\", **kwargs):\n",
    "        \"\"\"\n",
    "        Hook called before the forward pass of the model during testing.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        inputs = strategy.mbatch[0]  # Inputs (features)\n",
    "        labels = strategy.mbatch[1]  # Labels (targets)\n",
    "\n",
    "        # Package inputs and labels in the format expected by the model\n",
    "        packaged_input = (inputs, labels)\n",
    "\n",
    "        # Set packaged_input\n",
    "        strategy.mbatch[0] = packaged_input\n",
    "        strategy.mbatch[0] = packaged_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1520d633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torchvision.transforms as transforms\n",
    "from avalanche.benchmarks.classic import SplitMNIST, SplitCIFAR100,SplitTinyImageNet, CORe50\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics, \\\n",
    "    loss_metrics, timing_metrics, cpu_usage_metrics, confusion_matrix_metrics, disk_usage_metrics\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger, WandBLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.training.supervised import Naive\n",
    "from avalanche.training.templates import SupervisedTemplate\n",
    "from avalanche.training.plugins import ReplayPlugin, EWCPlugin, LRSchedulerPlugin\n",
    "from avalanche.training.plugins import BiCPlugin\n",
    "\n",
    "\n",
    "# there's a bug in wandb logging of the last iteration, we have to mannually finish it\n",
    "import wandb\n",
    "import torch.optim as optim\n",
    "import warmup_scheduler\n",
    "# scenario = SplitMNIST(n_experiences=5, seed = 1234)\n",
    "\n",
    "\n",
    "\n",
    "# scenario = SplitCIFAR100(n_experiences=10, seed = 1234)\n",
    "# scenario = SplitTinyImageNet(n_experiences=10, seed = 1234)\n",
    "scenario = CORe50(mini=True)\n",
    "device = 'cuda:0'\n",
    "# MODEL CREATION\n",
    "\n",
    "# model = ViT(\n",
    "#     image_size = 28,\n",
    "#     patch_size = 7,\n",
    "#     num_classes = 10,\n",
    "#     dim = 512,\n",
    "#     depth = 3,\n",
    "#     heads = 8,\n",
    "#     mlp_dim = 256,\n",
    "#     dropout = 0.1,\n",
    "#     emb_dropout = 0.1,\n",
    "#     img_channels = 1,\n",
    "#     feature_channels = 1,\n",
    "#     dim_head = 64\n",
    "# )\n",
    "\n",
    "model = ViT(\n",
    "    image_size = 32,\n",
    "    patch_size = 4,\n",
    "    num_classes = 100,\n",
    "    dim = 384,\n",
    "    depth = 6,\n",
    "    heads = 12,\n",
    "    mlp_dim = 384,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1,\n",
    "    img_channels = 3,\n",
    "    feature_channels = 3,\n",
    "    dim_head = 32,\n",
    "    device=device\n",
    ")\n",
    "# DEFINE THE EVALUATION PLUGIN and LOGGERS\n",
    "# The evaluation plugin manages the metrics computation.\n",
    "# It takes as argument a list of metrics, collectes their results and returns\n",
    "# them to the strategy it is attached to.\n",
    "\n",
    "loggers = []\n",
    "\n",
    "# log to Tensorboard\n",
    "# log to text file\n",
    "loggers.append(TextLogger(open('log.txt', 'a')))\n",
    "\n",
    "# print to stdout\n",
    "loggers.append(InteractiveLogger())\n",
    "\n",
    "# W&B logger - comment this if you don't have a W&B account\n",
    "loggers.append(WandBLogger(project_name=\"avalanche\", run_name=\"CIFAR100-input-nolabel\"))\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(epoch=True, epoch_running=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    confusion_matrix_metrics(num_classes=scenario.n_classes, save_image=False,\n",
    "                             stream=True),\n",
    "    loggers=loggers\n",
    ")\n",
    "\n",
    "\n",
    "# strategy plugign\n",
    "replay = ReplayPlugin(mem_size=1000)\n",
    "fsml = FewShotMetaLearningPlugin()\n",
    "ewc = EWCPlugin(ewc_lambda=1)\n",
    "bic = BiCPlugin(mem_size=1000)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3, weight_decay = 0.00005)\n",
    "# Assuming your optimizer is already defined\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 10, eta_min=1e-4)\n",
    "lr_scheduler_plugin = LRSchedulerPlugin(scheduler)\n",
    "\n",
    "# CREATE THE STRATEGY INSTANCE\n",
    "cl_strategy = SupervisedTemplate(\n",
    "    model, optimizer,\n",
    "    CrossEntropyLoss(), train_mb_size=128, train_epochs=10, eval_mb_size=128, device=device,\n",
    "    evaluator=eval_plugin,plugins=[fsml, replay, lr_scheduler_plugin], eval_every=1)\n",
    "\n",
    "\n",
    "\n",
    "# TRAINING LOOP\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "for i,experience in enumerate(scenario.train_stream[:]):\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    # train returns a dictionary which contains all the metric values\n",
    "    res = cl_strategy.train(experience, eval_streams=[scenario.test_stream[:i+1]])\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the whole test set')\n",
    "    # test also returns a dictionary which contains all the metric values\n",
    "    results.append(cl_strategy.eval(scenario.test_stream[:i+1]))\n",
    "\n",
    "# there's a bug in wandb logging of the last iteration, we have to mannually finish it\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ed8f8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, channels, hidden_dim):\n",
    "        super().__init__()\n",
    "        # Enhanced CNN architecture\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # First convolution block\n",
    "            nn.Conv2d(channels, hidden_dim//2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim//2, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_layers(x)\n",
    "\n",
    "class ResNet18FeatureExtractor(nn.Module):\n",
    "    def __init__(self, pretrained=False):\n",
    "        super().__init__()\n",
    "        # Load a pre-trained ResNet-18 model\n",
    "        self.resnet18 = models.resnet18(pretrained=pretrained)\n",
    "\n",
    "        # Remove the fully connected layer\n",
    "        # Alternatively, you can use AdaptiveAvgPool2d to control the output size\n",
    "        self.resnet18 = nn.Sequential(*list(self.resnet18.children())[:-2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet18(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.downsample = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class SmallResNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # BasicBlock with downsampling to reduce spatial dimensions\n",
    "        self.layer1 = BasicBlock(16, 32, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
    "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "\n",
    "        return self.norm(x)\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', img_channels = 1, feature_channels = 1, dim_head = 64, dropout = 0., emb_dropout = 0., n_examples = 1, device = 'cpu'):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = feature_channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "        ### positional embedding options: 1. learned 2. sinusoidal ###\n",
    "        # self.pos_embedding = nn.Parameter(torch.randn(1, (num_patches*2 + 3)*(examples), dim))\n",
    "        if n_examples == 0:\n",
    "            self.pos_embedding = self.sinusoidal_embeddings(num_patches * n_examples + 1, dim).to(device)\n",
    "        else:\n",
    "            self.pos_embedding = self.sinusoidal_embeddings((num_patches*2  + 4) * n_examples, dim).to(device)\n",
    "\n",
    "        self.comma_token = nn.Parameter(torch.randn(1, 1, dim, device= device)) # Token for ','\n",
    "        self.arrow_token = nn.Parameter(torch.randn(1, 1, dim, device= device))  # Token for '->'\n",
    "        self.pipe_token = nn.Parameter(torch.randn(1, 1, dim, device= device))   # Token for '|'\n",
    "        self.clf_token = nn.Parameter(torch.randn(1, 1, dim, device= device))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "        self.cnn_feature_extractor = CNNFeatureExtractor(img_channels, feature_channels)\n",
    "        self.resnet_feature_extractor =  SmallResNetFeatureExtractor()\n",
    "\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Linear(dim, num_classes)\n",
    "        self.n_examples = n_examples\n",
    "        self.dim = dim\n",
    "\n",
    "        self.saved_imgs = None\n",
    "        self.saved_labels = None\n",
    "        self.device = device\n",
    "\n",
    "    def sinusoidal_embeddings(self, n_pos, dim):\n",
    "        position = torch.arange(0, n_pos, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))\n",
    "        sinusoidal_emb = torch.zeros(n_pos, dim)\n",
    "        sinusoidal_emb[:, 0::2] = torch.sin(position * div_term)\n",
    "        sinusoidal_emb[:, 1::2] = torch.cos(position * div_term)\n",
    "        return sinusoidal_emb.unsqueeze(0)\n",
    "\n",
    "    def forward(self, mbatch):\n",
    "\n",
    "        ## mbatch contains labels\n",
    "        if mbatch[0].dim() == 4:\n",
    "            imgs = mbatch[0]\n",
    "            labels = mbatch[1]\n",
    "            n_examples = self.n_examples\n",
    "        ## mbatch contains images only\n",
    "        else:\n",
    "            imgs = mbatch\n",
    "            n_examples = 0\n",
    "\n",
    "\n",
    "        batch_size, _, _, _ = imgs.shape\n",
    "\n",
    "        seq = []\n",
    "\n",
    "         ## training and testing without exemples, the pipe token can serve as the cls token used for ViT classification\n",
    "        if n_examples == 0:\n",
    "            seq.append(repeat(self.pipe_token, '1 1 d -> b 1 d', b=batch_size))\n",
    "            seq.append(self.to_patch_embedding(imgs))\n",
    "        else:\n",
    "            if self.training:\n",
    "\n",
    "                if self.saved_imgs == None:\n",
    "                    self.saved_imgs = imgs[:n_examples]\n",
    "                    self.saved_labels = labels[:n_examples]\n",
    "\n",
    "                for i in range(n_examples):\n",
    "                    # Shift images and labels by 1 index\n",
    "                    # shifting and append aligns instances with other instances in the same batch, while these instances serve as the examples\n",
    "                    shifted_imgs = torch.roll(imgs, shifts=-i-1, dims=0)\n",
    "                    shifted_labels = torch.roll(labels, shifts=-i-1, dims=0)\n",
    "\n",
    "                    # # Convert labels to one-hot with length equal to embedding dimension (self.dim)\n",
    "                    shifted_labels = nn.functional.one_hot(shifted_labels, num_classes=self.dim).float()\n",
    "                    # append example (image, delimitor, label)\n",
    "                    seq.append(self.to_patch_embedding(shifted_imgs))\n",
    "                    seq.append(repeat(self.arrow_token, '1 1 d -> b 1 d', b=batch_size))\n",
    "                    seq.append(shifted_labels.unsqueeze(1))\n",
    "\n",
    "                    # append target (image)\n",
    "                    seq.append(repeat(self.pipe_token, '1 1 d -> b 1 d', b=batch_size))\n",
    "                    seq.append(self.to_patch_embedding(imgs))\n",
    "                    seq.append(repeat(self.clf_token, '1 1 d -> b 1 d', b=batch_size))\n",
    "\n",
    "            else:\n",
    "                if self.saved_imgs == None:\n",
    "                    self.saved_imgs = imgs[:n_examples]\n",
    "                    self.saved_labels = labels[:n_examples]\n",
    "                for i in range(n_examples):\n",
    "                    # Shift images and labels by 1 index\n",
    "                    # shifting and append aligns instances with other instances in the same batch, while these instances serve as the examples\n",
    "                    saved_img = self.saved_imgs[i].repeat(batch_size,1,1,1)\n",
    "                    saved_label = nn.functional.one_hot(self.saved_labels[i].repeat(batch_size,), num_classes=self.dim).float()\n",
    "\n",
    "                    seq.append(self.to_patch_embedding(saved_img))\n",
    "                    seq.append(repeat(self.arrow_token, '1 1 d -> b 1 d', b=batch_size))\n",
    "                    seq.append(saved_label.unsqueeze(1))\n",
    "                    # append target (image)\n",
    "                    seq.append(repeat(self.pipe_token, '1 1 d -> b 1 d', b=batch_size))\n",
    "                    seq.append(self.to_patch_embedding(imgs))\n",
    "                    seq.append(repeat(self.clf_token, '1 1 d -> b 1 d', b=batch_size))\n",
    "\n",
    "        seq = torch.cat(seq, dim=1)\n",
    "\n",
    "        # Add positional embeddings, ensure they match the sequence length\n",
    "        x = seq + self.pos_embedding[:, :seq.size(1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        x = self.mlp_head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0fa7960",
   "metadata": {},
   "outputs": [],
   "source": [
    "import avalanche\n",
    "from avalanche.core import SupervisedPlugin\n",
    "class FewShotMetaLearningPlugin(SupervisedPlugin):\n",
    "    # ... other methods ...\n",
    "\n",
    "    def before_forward(self, strategy: \"SupervisedTemplate\", **kwargs):\n",
    "        \"\"\"\n",
    "        Hook called before the forward pass of the model during training.\n",
    "        Our model takes additional label information, we provide it as a (inputs, labels) tuple\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = strategy.mbatch[0]  # Inputs (features)\n",
    "        labels = strategy.mbatch[1]  # Labels (targets)\n",
    "\n",
    "        # Package inputs and labels in the format expected by the model\n",
    "        packaged_input = (inputs, labels)\n",
    "\n",
    "        # Set packaged_input, check the api document for other available controls. mbatch is sent as the only argument taken in the forward function\n",
    "        strategy.mbatch[0] = packaged_input\n",
    "\n",
    "    def before_eval_forward(self, strategy: \"SupervisedTemplate\", **kwargs):\n",
    "        \"\"\"\n",
    "        Hook called before the forward pass of the model during testing.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        inputs = strategy.mbatch[0]  # Inputs (features)\n",
    "        labels = strategy.mbatch[1]  # Labels (targets)\n",
    "\n",
    "        # Package inputs and labels in the format expected by the model\n",
    "        packaged_input = (inputs, labels)\n",
    "\n",
    "        # Set packaged_input\n",
    "        strategy.mbatch[0] = packaged_input\n",
    "        strategy.mbatch[0] = packaged_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d95f4a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torchvision.transforms as transforms\n",
    "from avalanche.benchmarks.classic import SplitMNIST, SplitCIFAR100,SplitTinyImageNet, CORe50\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics, \\\n",
    "    loss_metrics, timing_metrics, cpu_usage_metrics, confusion_matrix_metrics, disk_usage_metrics\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger, WandBLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.training.supervised import Naive\n",
    "from avalanche.training.templates import SupervisedTemplate\n",
    "from avalanche.training.plugins import ReplayPlugin, EWCPlugin, LRSchedulerPlugin\n",
    "from avalanche.training.plugins import BiCPlugin\n",
    "\n",
    "\n",
    "# there's a bug in wandb logging of the last iteration, we have to mannually finish it\n",
    "import wandb\n",
    "import torch.optim as optim\n",
    "import warmup_scheduler\n",
    "# scenario = SplitMNIST(n_experiences=5, seed = 1234)\n",
    "\n",
    "\n",
    "\n",
    "# scenario = SplitCIFAR100(n_experiences=10, seed = 1234)\n",
    "# scenario = SplitTinyImageNet(n_experiences=10, seed = 1234)\n",
    "scenario = CORe50(mini=True)\n",
    "device = 'cuda:0'\n",
    "# MODEL CREATION\n",
    "\n",
    "# model = ViT(\n",
    "#     image_size = 28,\n",
    "#     patch_size = 7,\n",
    "#     num_classes = 10,\n",
    "#     dim = 512,\n",
    "#     depth = 3,\n",
    "#     heads = 8,\n",
    "#     mlp_dim = 256,\n",
    "#     dropout = 0.1,\n",
    "#     emb_dropout = 0.1,\n",
    "#     img_channels = 1,\n",
    "#     feature_channels = 1,\n",
    "#     dim_head = 64\n",
    "# )\n",
    "\n",
    "model = ViT(\n",
    "    image_size = 32,\n",
    "    patch_size = 4,\n",
    "    num_classes = 100,\n",
    "    dim = 384,\n",
    "    depth = 6,\n",
    "    heads = 12,\n",
    "    mlp_dim = 384,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1,\n",
    "    img_channels = 3,\n",
    "    feature_channels = 3,\n",
    "    dim_head = 32,\n",
    "    device=device\n",
    ")\n",
    "# DEFINE THE EVALUATION PLUGIN and LOGGERS\n",
    "# The evaluation plugin manages the metrics computation.\n",
    "# It takes as argument a list of metrics, collectes their results and returns\n",
    "# them to the strategy it is attached to.\n",
    "\n",
    "loggers = []\n",
    "\n",
    "# log to Tensorboard\n",
    "# log to text file\n",
    "loggers.append(TextLogger(open('log.txt', 'a')))\n",
    "\n",
    "# print to stdout\n",
    "loggers.append(InteractiveLogger())\n",
    "\n",
    "# W&B logger - comment this if you don't have a W&B account\n",
    "loggers.append(WandBLogger(project_name=\"avalanche\", run_name=\"CIFAR100-input-nolabel\"))\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(epoch=True, epoch_running=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    confusion_matrix_metrics(num_classes=scenario.n_classes, save_image=False,\n",
    "                             stream=True),\n",
    "    loggers=loggers\n",
    ")\n",
    "\n",
    "\n",
    "# strategy plugign\n",
    "replay = ReplayPlugin(mem_size=1000)\n",
    "fsml = FewShotMetaLearningPlugin()\n",
    "ewc = EWCPlugin(ewc_lambda=1)\n",
    "bic = BiCPlugin(mem_size=1000)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3, weight_decay = 0.00005)\n",
    "# Assuming your optimizer is already defined\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 10, eta_min=1e-4)\n",
    "lr_scheduler_plugin = LRSchedulerPlugin(scheduler)\n",
    "\n",
    "# CREATE THE STRATEGY INSTANCE\n",
    "cl_strategy = SupervisedTemplate(\n",
    "    model, optimizer,\n",
    "    CrossEntropyLoss(), train_mb_size=128, train_epochs=10, eval_mb_size=128, device=device,\n",
    "    evaluator=eval_plugin,plugins=[fsml, replay, lr_scheduler_plugin], eval_every=1)\n",
    "\n",
    "\n",
    "\n",
    "# TRAINING LOOP\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "for i,experience in enumerate(scenario.train_stream[:]):\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    # train returns a dictionary which contains all the metric values\n",
    "    res = cl_strategy.train(experience, eval_streams=[scenario.test_stream[:i+1]])\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the whole test set')\n",
    "    # test also returns a dictionary which contains all the metric values\n",
    "    results.append(cl_strategy.eval(scenario.test_stream[:i+1]))\n",
    "\n",
    "# there's a bug in wandb logging of the last iteration, we have to mannually finish it\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1371e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# helpers\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "# classes\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.attend = nn.Softmax(dim = -1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        return self.to_out(out)\n",
    "\n",
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, channels, hidden_dim):\n",
    "        super().__init__()\n",
    "        # Enhanced CNN architecture\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            # First convolution block\n",
    "            nn.Conv2d(channels, hidden_dim//2, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_dim//2, hidden_dim, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_layers(x)\n",
    "\n",
    "class ResNet18FeatureExtractor(nn.Module):\n",
    "    def __init__(self, pretrained=False):\n",
    "        super().__init__()\n",
    "        # Load a pre-trained ResNet-18 model\n",
    "        self.resnet18 = models.resnet18(pretrained=pretrained)\n",
    "\n",
    "        # Remove the fully connected layer\n",
    "        # Alternatively, you can use AdaptiveAvgPool2d to control the output size\n",
    "        self.resnet18 = nn.Sequential(*list(self.resnet18.children())[:-2])\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet18(x)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.downsample = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.downsample(x)\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class SmallResNetFeatureExtractor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        # BasicBlock with downsampling to reduce spatial dimensions\n",
    "        self.layer1 = BasicBlock(16, 32, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout),\n",
    "                FeedForward(dim, mlp_dim, dropout = dropout)\n",
    "            ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "\n",
    "        return self.norm(x)\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, *, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim, pool = 'cls', img_channels = 1, feature_channels = 1, dim_head = 64, dropout = 0., emb_dropout = 0., n_examples = 1, device = 'cpu'):\n",
    "        super().__init__()\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "        patch_dim = feature_channels * patch_height * patch_width\n",
    "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.LayerNorm(patch_dim),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "            nn.LayerNorm(dim),\n",
    "        )\n",
    "        ### positional embedding options: 1. learned 2. sinusoidal ###\n",
    "        # self.pos_embedding = nn.Parameter(torch.randn(1, (num_patches*2 + 3)*(examples), dim))\n",
    "        if n_examples == 0:\n",
    "            self.pos_embedding = self.sinusoidal_embeddings(num_patches * n_examples + 1, dim).to(device)\n",
    "        else:\n",
    "            self.pos_embedding = self.sinusoidal_embeddings((num_patches*2  + 4) * n_examples, dim).to(device)\n",
    "\n",
    "        self.comma_token = nn.Parameter(torch.randn(1, 1, dim, device= device)) # Token for ','\n",
    "        self.arrow_token = nn.Parameter(torch.randn(1, 1, dim, device= device))  # Token for '->'\n",
    "        self.pipe_token = nn.Parameter(torch.randn(1, 1, dim, device= device))   # Token for '|'\n",
    "        self.clf_token = nn.Parameter(torch.randn(1, 1, dim, device= device))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "\n",
    "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
    "        self.cnn_feature_extractor = CNNFeatureExtractor(img_channels, feature_channels)\n",
    "        self.resnet_feature_extractor =  SmallResNetFeatureExtractor()\n",
    "\n",
    "\n",
    "        self.pool = pool\n",
    "        self.to_latent = nn.Identity()\n",
    "\n",
    "        self.mlp_head = nn.Linear(dim, num_classes)\n",
    "        self.n_examples = n_examples\n",
    "        self.dim = dim\n",
    "\n",
    "        self.saved_imgs = None\n",
    "        self.saved_labels = None\n",
    "        self.device = device\n",
    "\n",
    "    def sinusoidal_embeddings(self, n_pos, dim):\n",
    "        position = torch.arange(0, n_pos, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, dim, 2).float() * (-math.log(10000.0) / dim))\n",
    "        sinusoidal_emb = torch.zeros(n_pos, dim)\n",
    "        sinusoidal_emb[:, 0::2] = torch.sin(position * div_term)\n",
    "        sinusoidal_emb[:, 1::2] = torch.cos(position * div_term)\n",
    "        return sinusoidal_emb.unsqueeze(0)\n",
    "\n",
    "    def forward(self, mbatch):\n",
    "\n",
    "        ## mbatch contains labels\n",
    "        if mbatch[0].dim() == 4:\n",
    "            imgs = mbatch[0]\n",
    "            labels = mbatch[1]\n",
    "            n_examples = self.n_examples\n",
    "        ## mbatch contains images only\n",
    "        else:\n",
    "            imgs = mbatch\n",
    "            n_examples = 0\n",
    "\n",
    "\n",
    "        batch_size, _, _, _ = imgs.shape\n",
    "\n",
    "        seq = []\n",
    "\n",
    "         ## training and testing without exemples, the pipe token can serve as the cls token used for ViT classification\n",
    "        if n_examples == 0:\n",
    "            seq.append(repeat(self.pipe_token, '1 1 d -> b 1 d', b=batch_size))\n",
    "            seq.append(self.to_patch_embedding(imgs))\n",
    "        else:\n",
    "            if self.training:\n",
    "\n",
    "                if self.saved_imgs == None:\n",
    "                    self.saved_imgs = imgs[:n_examples]\n",
    "                    self.saved_labels = labels[:n_examples]\n",
    "\n",
    "                for i in range(n_examples):\n",
    "                    # Shift images and labels by 1 index\n",
    "                    # shifting and append aligns instances with other instances in the same batch, while these instances serve as the examples\n",
    "                    shifted_imgs = torch.roll(imgs, shifts=-i-1, dims=0)\n",
    "                    shifted_labels = torch.roll(labels, shifts=-i-1, dims=0)\n",
    "\n",
    "                    # # Convert labels to one-hot with length equal to embedding dimension (self.dim)\n",
    "                    shifted_labels = nn.functional.one_hot(shifted_labels, num_classes=self.dim).float()\n",
    "                    # append example (image, delimitor, label)\n",
    "                    seq.append(self.to_patch_embedding(shifted_imgs))\n",
    "                    seq.append(repeat(self.arrow_token, '1 1 d -> b 1 d', b=batch_size))\n",
    "                    seq.append(shifted_labels.unsqueeze(1))\n",
    "\n",
    "                    # append target (image)\n",
    "                    seq.append(repeat(self.pipe_token, '1 1 d -> b 1 d', b=batch_size))\n",
    "                    seq.append(self.to_patch_embedding(imgs))\n",
    "                    # seq.append(repeat(self.clf_token, '1 1 d -> b 1 d', b=batch_size))\n",
    "\n",
    "            else:\n",
    "                if self.saved_imgs == None:\n",
    "                    self.saved_imgs = imgs[:n_examples]\n",
    "                    self.saved_labels = labels[:n_examples]\n",
    "                for i in range(n_examples):\n",
    "                    # Shift images and labels by 1 index\n",
    "                    # shifting and append aligns instances with other instances in the same batch, while these instances serve as the examples\n",
    "                    saved_img = self.saved_imgs[i].repeat(batch_size,1,1,1)\n",
    "                    saved_label = nn.functional.one_hot(self.saved_labels[i].repeat(batch_size,), num_classes=self.dim).float()\n",
    "\n",
    "                    seq.append(self.to_patch_embedding(saved_img))\n",
    "                    seq.append(repeat(self.arrow_token, '1 1 d -> b 1 d', b=batch_size))\n",
    "                    seq.append(saved_label.unsqueeze(1))\n",
    "                    # append target (image)\n",
    "                    seq.append(repeat(self.pipe_token, '1 1 d -> b 1 d', b=batch_size))\n",
    "                    seq.append(self.to_patch_embedding(imgs))\n",
    "                    # seq.append(repeat(self.clf_token, '1 1 d -> b 1 d', b=batch_size))\n",
    "\n",
    "        seq = torch.cat(seq, dim=1)\n",
    "\n",
    "        # Add positional embeddings, ensure they match the sequence length\n",
    "        x = seq + self.pos_embedding[:, :seq.size(1)]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        x = x.mean(dim = 1) if self.pool == 'mean' else x[:, 0]\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        x = self.mlp_head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "864244e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import avalanche\n",
    "from avalanche.core import SupervisedPlugin\n",
    "class FewShotMetaLearningPlugin(SupervisedPlugin):\n",
    "    # ... other methods ...\n",
    "\n",
    "    def before_forward(self, strategy: \"SupervisedTemplate\", **kwargs):\n",
    "        \"\"\"\n",
    "        Hook called before the forward pass of the model during training.\n",
    "        Our model takes additional label information, we provide it as a (inputs, labels) tuple\n",
    "        \"\"\"\n",
    "\n",
    "        inputs = strategy.mbatch[0]  # Inputs (features)\n",
    "        labels = strategy.mbatch[1]  # Labels (targets)\n",
    "\n",
    "        # Package inputs and labels in the format expected by the model\n",
    "        packaged_input = (inputs, labels)\n",
    "\n",
    "        # Set packaged_input, check the api document for other available controls. mbatch is sent as the only argument taken in the forward function\n",
    "        strategy.mbatch[0] = packaged_input\n",
    "\n",
    "    def before_eval_forward(self, strategy: \"SupervisedTemplate\", **kwargs):\n",
    "        \"\"\"\n",
    "        Hook called before the forward pass of the model during testing.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        inputs = strategy.mbatch[0]  # Inputs (features)\n",
    "        labels = strategy.mbatch[1]  # Labels (targets)\n",
    "\n",
    "        # Package inputs and labels in the format expected by the model\n",
    "        packaged_input = (inputs, labels)\n",
    "\n",
    "        # Set packaged_input\n",
    "        strategy.mbatch[0] = packaged_input\n",
    "        strategy.mbatch[0] = packaged_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6be69338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torchvision.transforms as transforms\n",
    "from avalanche.benchmarks.classic import SplitMNIST, SplitCIFAR100,SplitTinyImageNet, CORe50\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics, \\\n",
    "    loss_metrics, timing_metrics, cpu_usage_metrics, confusion_matrix_metrics, disk_usage_metrics\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger, WandBLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.training.supervised import Naive\n",
    "from avalanche.training.templates import SupervisedTemplate\n",
    "from avalanche.training.plugins import ReplayPlugin, EWCPlugin, LRSchedulerPlugin\n",
    "from avalanche.training.plugins import BiCPlugin\n",
    "\n",
    "\n",
    "# there's a bug in wandb logging of the last iteration, we have to mannually finish it\n",
    "import wandb\n",
    "import torch.optim as optim\n",
    "import warmup_scheduler\n",
    "# scenario = SplitMNIST(n_experiences=5, seed = 1234)\n",
    "\n",
    "\n",
    "\n",
    "# scenario = SplitCIFAR100(n_experiences=10, seed = 1234)\n",
    "# scenario = SplitTinyImageNet(n_experiences=10, seed = 1234)\n",
    "scenario = CORe50(mini=True)\n",
    "device = 'cuda:0'\n",
    "# MODEL CREATION\n",
    "\n",
    "# model = ViT(\n",
    "#     image_size = 28,\n",
    "#     patch_size = 7,\n",
    "#     num_classes = 10,\n",
    "#     dim = 512,\n",
    "#     depth = 3,\n",
    "#     heads = 8,\n",
    "#     mlp_dim = 256,\n",
    "#     dropout = 0.1,\n",
    "#     emb_dropout = 0.1,\n",
    "#     img_channels = 1,\n",
    "#     feature_channels = 1,\n",
    "#     dim_head = 64\n",
    "# )\n",
    "\n",
    "model = ViT(\n",
    "    image_size = 32,\n",
    "    patch_size = 4,\n",
    "    num_classes = 100,\n",
    "    dim = 384,\n",
    "    depth = 6,\n",
    "    heads = 12,\n",
    "    mlp_dim = 384,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1,\n",
    "    img_channels = 3,\n",
    "    feature_channels = 3,\n",
    "    dim_head = 32,\n",
    "    device=device\n",
    ")\n",
    "# DEFINE THE EVALUATION PLUGIN and LOGGERS\n",
    "# The evaluation plugin manages the metrics computation.\n",
    "# It takes as argument a list of metrics, collectes their results and returns\n",
    "# them to the strategy it is attached to.\n",
    "\n",
    "loggers = []\n",
    "\n",
    "# log to Tensorboard\n",
    "# log to text file\n",
    "loggers.append(TextLogger(open('log.txt', 'a')))\n",
    "\n",
    "# print to stdout\n",
    "loggers.append(InteractiveLogger())\n",
    "\n",
    "# W&B logger - comment this if you don't have a W&B account\n",
    "loggers.append(WandBLogger(project_name=\"avalanche\", run_name=\"CIFAR100-input-nolabel\"))\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(epoch=True, epoch_running=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    confusion_matrix_metrics(num_classes=scenario.n_classes, save_image=False,\n",
    "                             stream=True),\n",
    "    loggers=loggers\n",
    ")\n",
    "\n",
    "\n",
    "# strategy plugign\n",
    "replay = ReplayPlugin(mem_size=1000)\n",
    "fsml = FewShotMetaLearningPlugin()\n",
    "ewc = EWCPlugin(ewc_lambda=1)\n",
    "bic = BiCPlugin(mem_size=1000)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3, weight_decay = 0.00005)\n",
    "# Assuming your optimizer is already defined\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 10, eta_min=1e-4)\n",
    "lr_scheduler_plugin = LRSchedulerPlugin(scheduler)\n",
    "\n",
    "# CREATE THE STRATEGY INSTANCE\n",
    "cl_strategy = SupervisedTemplate(\n",
    "    model, optimizer,\n",
    "    CrossEntropyLoss(), train_mb_size=128, train_epochs=10, eval_mb_size=128, device=device,\n",
    "    evaluator=eval_plugin,plugins=[fsml, replay, lr_scheduler_plugin], eval_every=1)\n",
    "\n",
    "\n",
    "\n",
    "# TRAINING LOOP\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "for i,experience in enumerate(scenario.train_stream[:]):\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    # train returns a dictionary which contains all the metric values\n",
    "    res = cl_strategy.train(experience, eval_streams=[scenario.test_stream[:i+1]])\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the whole test set')\n",
    "    # test also returns a dictionary which contains all the metric values\n",
    "    results.append(cl_strategy.eval(scenario.test_stream[:i+1]))\n",
    "\n",
    "# there's a bug in wandb logging of the last iteration, we have to mannually finish it\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05dbc8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torchvision.transforms as transforms\n",
    "from avalanche.benchmarks.classic import SplitMNIST, SplitCIFAR100,SplitTinyImageNet, CORe50\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics, \\\n",
    "    loss_metrics, timing_metrics, cpu_usage_metrics, confusion_matrix_metrics, disk_usage_metrics\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger, WandBLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.training.supervised import Naive\n",
    "from avalanche.training.templates import SupervisedTemplate\n",
    "from avalanche.training.plugins import ReplayPlugin, EWCPlugin, LRSchedulerPlugin\n",
    "from avalanche.training.plugins import BiCPlugin\n",
    "\n",
    "\n",
    "# there's a bug in wandb logging of the last iteration, we have to mannually finish it\n",
    "import wandb\n",
    "import torch.optim as optim\n",
    "import warmup_scheduler\n",
    "# scenario = SplitMNIST(n_experiences=5, seed = 1234)\n",
    "\n",
    "\n",
    "\n",
    "# scenario = SplitCIFAR100(n_experiences=10, seed = 1234)\n",
    "# scenario = SplitTinyImageNet(n_experiences=10, seed = 1234)\n",
    "scenario = CORe50(mini=True)\n",
    "device = 'cuda:0'\n",
    "# MODEL CREATION\n",
    "\n",
    "# model = ViT(\n",
    "#     image_size = 28,\n",
    "#     patch_size = 7,\n",
    "#     num_classes = 10,\n",
    "#     dim = 512,\n",
    "#     depth = 3,\n",
    "#     heads = 8,\n",
    "#     mlp_dim = 256,\n",
    "#     dropout = 0.1,\n",
    "#     emb_dropout = 0.1,\n",
    "#     img_channels = 1,\n",
    "#     feature_channels = 1,\n",
    "#     dim_head = 64\n",
    "# )\n",
    "\n",
    "model = ViT(\n",
    "    image_size = 32,\n",
    "    patch_size = 4,\n",
    "    num_classes = 100,\n",
    "    dim = 384,\n",
    "    depth = 6,\n",
    "    heads = 12,\n",
    "    mlp_dim = 384,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1,\n",
    "    img_channels = 3,\n",
    "    feature_channels = 3,\n",
    "    dim_head = 32,\n",
    "    device=device\n",
    ")\n",
    "# DEFINE THE EVALUATION PLUGIN and LOGGERS\n",
    "# The evaluation plugin manages the metrics computation.\n",
    "# It takes as argument a list of metrics, collectes their results and returns\n",
    "# them to the strategy it is attached to.\n",
    "\n",
    "loggers = []\n",
    "\n",
    "# log to Tensorboard\n",
    "# log to text file\n",
    "loggers.append(TextLogger(open('log.txt', 'a')))\n",
    "\n",
    "# print to stdout\n",
    "loggers.append(InteractiveLogger())\n",
    "\n",
    "# W&B logger - comment this if you don't have a W&B account\n",
    "loggers.append(WandBLogger(project_name=\"avalanche\", run_name=\"CIFAR100-input-nolabel\"))\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(epoch=True, epoch_running=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    confusion_matrix_metrics(num_classes=scenario.n_classes, save_image=False,\n",
    "                             stream=True),\n",
    "    loggers=loggers\n",
    ")\n",
    "\n",
    "\n",
    "# strategy plugign\n",
    "replay = ReplayPlugin(mem_size=1000)\n",
    "fsml = FewShotMetaLearningPlugin()\n",
    "ewc = EWCPlugin(ewc_lambda=1)\n",
    "bic = BiCPlugin(mem_size=1000)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3, weight_decay = 0.00005)\n",
    "# Assuming your optimizer is already defined\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 10, eta_min=1e-4)\n",
    "lr_scheduler_plugin = LRSchedulerPlugin(scheduler)\n",
    "\n",
    "# CREATE THE STRATEGY INSTANCE\n",
    "cl_strategy = SupervisedTemplate(\n",
    "    model, optimizer,\n",
    "    CrossEntropyLoss(), train_mb_size=128, train_epochs=10, eval_mb_size=128, device=device,\n",
    "    evaluator=eval_plugin,plugins=[fsml, replay, lr_scheduler_plugin], eval_every=5)\n",
    "\n",
    "\n",
    "\n",
    "# TRAINING LOOP\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "for i,experience in enumerate(scenario.train_stream[:]):\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    # train returns a dictionary which contains all the metric values\n",
    "    res = cl_strategy.train(experience, eval_streams=[scenario.test_stream[:i+1]])\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the whole test set')\n",
    "    # test also returns a dictionary which contains all the metric values\n",
    "    results.append(cl_strategy.eval(scenario.test_stream[:i+1]))\n",
    "\n",
    "# there's a bug in wandb logging of the last iteration, we have to mannually finish it\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed0d3a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ol9ea7ic) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>RunningTime_Epoch/train_phase/train_stream/Task000</td><td>█▃▂▁▁█▂▂▁▁▄▂▁▁▁▃▂▁▁▇▂▂▁▁▄▂▁▁▁▃▂▁▁▇▂▂▁▁▄▂</td></tr><tr><td>StreamForgetting/eval_phase/test_stream</td><td>▁▁▁</td></tr><tr><td>Time_Epoch/train_phase/train_stream/Task000</td><td>██▂▁▇▆▄▅</td></tr><tr><td>Top1_Acc_Epoch/train_phase/train_stream/Task000</td><td>▁▃▆▇▇███</td></tr><tr><td>Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp000</td><td>▁▃█</td></tr><tr><td>Top1_Acc_Stream/eval_phase/test_stream/Task000</td><td>▁▃█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>RunningTime_Epoch/train_phase/train_stream/Task000</td><td>0.00727</td></tr><tr><td>StreamForgetting/eval_phase/test_stream</td><td>0.0</td></tr><tr><td>Time_Epoch/train_phase/train_stream/Task000</td><td>1.84318</td></tr><tr><td>Top1_Acc_Epoch/train_phase/train_stream/Task000</td><td>0.98466</td></tr><tr><td>Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp000</td><td>0.06191</td></tr><tr><td>Top1_Acc_Stream/eval_phase/test_stream/Task000</td><td>0.06191</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">CIFAR100-input-nolabel</strong> at: <a href='https://wandb.ai/meta-class-incremental-learning/avalanche/runs/ol9ea7ic' target=\"_blank\">https://wandb.ai/meta-class-incremental-learning/avalanche/runs/ol9ea7ic</a><br/>Synced 7 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240213_224838-ol9ea7ic/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ol9ea7ic). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/zizhao/development/U-ViT/wandb/run-20240213_225041-8q7icdu1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/meta-class-incremental-learning/avalanche/runs/8q7icdu1' target=\"_blank\">CIFAR100-input-nolabel</a></strong> to <a href='https://wandb.ai/meta-class-incremental-learning/avalanche' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/meta-class-incremental-learning/avalanche' target=\"_blank\">https://wandb.ai/meta-class-incremental-learning/avalanche</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/meta-class-incremental-learning/avalanche/runs/8q7icdu1' target=\"_blank\">https://wandb.ai/meta-class-incremental-learning/avalanche/runs/8q7icdu1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torchvision.transforms as transforms\n",
    "from avalanche.benchmarks.classic import SplitMNIST, SplitCIFAR100,SplitTinyImageNet, CORe50\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics, \\\n",
    "    loss_metrics, timing_metrics, cpu_usage_metrics, confusion_matrix_metrics, disk_usage_metrics\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger, WandBLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.training.supervised import Naive\n",
    "from avalanche.training.templates import SupervisedTemplate\n",
    "from avalanche.training.plugins import ReplayPlugin, EWCPlugin, LRSchedulerPlugin\n",
    "from avalanche.training.plugins import BiCPlugin\n",
    "\n",
    "\n",
    "# there's a bug in wandb logging of the last iteration, we have to mannually finish it\n",
    "import wandb\n",
    "import torch.optim as optim\n",
    "import warmup_scheduler\n",
    "# scenario = SplitMNIST(n_experiences=5, seed = 1234)\n",
    "\n",
    "\n",
    "\n",
    "scenario = SplitCIFAR100(n_experiences=10, seed = 1234)\n",
    "# scenario = SplitTinyImageNet(n_experiences=10, seed = 1234)\n",
    "# scenario = CORe50(mini=True)\n",
    "device = 'cuda:0'\n",
    "# MODEL CREATION\n",
    "\n",
    "# model = ViT(\n",
    "#     image_size = 28,\n",
    "#     patch_size = 7,\n",
    "#     num_classes = 10,\n",
    "#     dim = 512,\n",
    "#     depth = 3,\n",
    "#     heads = 8,\n",
    "#     mlp_dim = 256,\n",
    "#     dropout = 0.1,\n",
    "#     emb_dropout = 0.1,\n",
    "#     img_channels = 1,\n",
    "#     feature_channels = 1,\n",
    "#     dim_head = 64\n",
    "# )\n",
    "\n",
    "model = ViT(\n",
    "    image_size = 32,\n",
    "    patch_size = 4,\n",
    "    num_classes = 100,\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 384,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1,\n",
    "    img_channels = 3,\n",
    "    feature_channels = 3,\n",
    "    dim_head = 32,\n",
    "    device=device\n",
    ")\n",
    "# DEFINE THE EVALUATION PLUGIN and LOGGERS\n",
    "# The evaluation plugin manages the metrics computation.\n",
    "# It takes as argument a list of metrics, collectes their results and returns\n",
    "# them to the strategy it is attached to.\n",
    "\n",
    "loggers = []\n",
    "\n",
    "# log to Tensorboard\n",
    "# log to text file\n",
    "loggers.append(TextLogger(open('log.txt', 'a')))\n",
    "\n",
    "# print to stdout\n",
    "loggers.append(InteractiveLogger())\n",
    "\n",
    "# W&B logger - comment this if you don't have a W&B account\n",
    "loggers.append(WandBLogger(project_name=\"avalanche\", run_name=\"CIFAR100-input-nolabel\"))\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(epoch=True, epoch_running=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    confusion_matrix_metrics(num_classes=scenario.n_classes, save_image=False,\n",
    "                             stream=True),\n",
    "    loggers=loggers\n",
    ")\n",
    "\n",
    "\n",
    "# strategy plugign\n",
    "replay = ReplayPlugin(mem_size=1000)\n",
    "fsml = FewShotMetaLearningPlugin()\n",
    "ewc = EWCPlugin(ewc_lambda=1)\n",
    "bic = BiCPlugin(mem_size=1000)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3, weight_decay = 0.00005)\n",
    "# Assuming your optimizer is already defined\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 10, eta_min=1e-4)\n",
    "lr_scheduler_plugin = LRSchedulerPlugin(scheduler)\n",
    "\n",
    "# CREATE THE STRATEGY INSTANCE\n",
    "cl_strategy = SupervisedTemplate(\n",
    "    model, optimizer,\n",
    "    CrossEntropyLoss(), train_mb_size=128, train_epochs=10, eval_mb_size=128, device=device,\n",
    "    evaluator=eval_plugin,plugins=[fsml, replay, lr_scheduler_plugin], eval_every=5)\n",
    "\n",
    "\n",
    "\n",
    "# TRAINING LOOP\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "for i,experience in enumerate(scenario.train_stream[:]):\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    # train returns a dictionary which contains all the metric values\n",
    "    res = cl_strategy.train(experience, eval_streams=[scenario.test_stream[:i+1]])\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the whole test set')\n",
    "    # test also returns a dictionary which contains all the metric values\n",
    "    results.append(cl_strategy.eval(scenario.test_stream[:i+1]))\n",
    "\n",
    "# there's a bug in wandb logging of the last iteration, we have to mannually finish it\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fcd62bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torchvision.transforms as transforms\n",
    "from avalanche.benchmarks.classic import SplitMNIST, SplitCIFAR100,SplitTinyImageNet, CORe50\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics, \\\n",
    "    loss_metrics, timing_metrics, cpu_usage_metrics, confusion_matrix_metrics, disk_usage_metrics\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger, WandBLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.training.supervised import Naive\n",
    "from avalanche.training.templates import SupervisedTemplate\n",
    "from avalanche.training.plugins import ReplayPlugin, EWCPlugin, LRSchedulerPlugin\n",
    "from avalanche.training.plugins import BiCPlugin\n",
    "\n",
    "\n",
    "# there's a bug in wandb logging of the last iteration, we have to mannually finish it\n",
    "import wandb\n",
    "import torch.optim as optim\n",
    "import warmup_scheduler\n",
    "# scenario = SplitMNIST(n_experiences=5, seed = 1234)\n",
    "\n",
    "\n",
    "\n",
    "# scenario = SplitCIFAR100(n_experiences=10, seed = 1234)\n",
    "# scenario = SplitTinyImageNet(n_experiences=10, seed = 1234)\n",
    "scenario = CORe50(mini=True)\n",
    "device = 'cuda:0'\n",
    "# MODEL CREATION\n",
    "\n",
    "# model = ViT(\n",
    "#     image_size = 28,\n",
    "#     patch_size = 7,\n",
    "#     num_classes = 10,\n",
    "#     dim = 512,\n",
    "#     depth = 3,\n",
    "#     heads = 8,\n",
    "#     mlp_dim = 256,\n",
    "#     dropout = 0.1,\n",
    "#     emb_dropout = 0.1,\n",
    "#     img_channels = 1,\n",
    "#     feature_channels = 1,\n",
    "#     dim_head = 64\n",
    "# )\n",
    "\n",
    "model = ViT(\n",
    "    image_size = 32,\n",
    "    patch_size = 4,\n",
    "    num_classes = 100,\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 384,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1,\n",
    "    img_channels = 3,\n",
    "    feature_channels = 3,\n",
    "    dim_head = 32,\n",
    "    device=device\n",
    ")\n",
    "# DEFINE THE EVALUATION PLUGIN and LOGGERS\n",
    "# The evaluation plugin manages the metrics computation.\n",
    "# It takes as argument a list of metrics, collectes their results and returns\n",
    "# them to the strategy it is attached to.\n",
    "\n",
    "loggers = []\n",
    "\n",
    "# log to Tensorboard\n",
    "# log to text file\n",
    "loggers.append(TextLogger(open('log.txt', 'a')))\n",
    "\n",
    "# print to stdout\n",
    "loggers.append(InteractiveLogger())\n",
    "\n",
    "# W&B logger - comment this if you don't have a W&B account\n",
    "loggers.append(WandBLogger(project_name=\"avalanche\", run_name=\"CIFAR100-input-nolabel\"))\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(epoch=True, epoch_running=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    confusion_matrix_metrics(num_classes=scenario.n_classes, save_image=False,\n",
    "                             stream=True),\n",
    "    loggers=loggers\n",
    ")\n",
    "\n",
    "\n",
    "# strategy plugign\n",
    "replay = ReplayPlugin(mem_size=1000)\n",
    "fsml = FewShotMetaLearningPlugin()\n",
    "ewc = EWCPlugin(ewc_lambda=1)\n",
    "bic = BiCPlugin(mem_size=1000)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3, weight_decay = 0.00005)\n",
    "# Assuming your optimizer is already defined\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 400, eta_min=1e-4)\n",
    "lr_scheduler_plugin = LRSchedulerPlugin(scheduler)\n",
    "\n",
    "# CREATE THE STRATEGY INSTANCE\n",
    "cl_strategy = SupervisedTemplate(\n",
    "    model, optimizer,\n",
    "    CrossEntropyLoss(), train_mb_size=128, train_epochs=160, eval_mb_size=128, device=device,\n",
    "    evaluator=eval_plugin,plugins=[fsml, replay, lr_scheduler_plugin]， eval_every=20)\n",
    "\n",
    "\n",
    "\n",
    "# TRAINING LOOP\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "for i,experience in enumerate(scenario.train_stream[:]):\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    # train returns a dictionary which contains all the metric values\n",
    "    if i == 0:\n",
    "        cl_strategy.train_epochs = 400\n",
    "    else:\n",
    "        cl_strategy.train_epochs = 160\n",
    "\n",
    "    res = cl_strategy.train(experience, eval_streams=[scenario.test_stream[:i+1]])\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the whole test set')\n",
    "    # test also returns a dictionary which contains all the metric values\n",
    "    results.append(cl_strategy.eval(scenario.test_stream[:i+1]))\n",
    "\n",
    "# there's a bug in wandb logging of the last iteration, we have to mannually finish it\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0503292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torchvision.transforms as transforms\n",
    "from avalanche.benchmarks.classic import SplitMNIST, SplitCIFAR100,SplitTinyImageNet, CORe50\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, accuracy_metrics, \\\n",
    "    loss_metrics, timing_metrics, cpu_usage_metrics, confusion_matrix_metrics, disk_usage_metrics\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.logging import InteractiveLogger, TextLogger, TensorboardLogger, WandBLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.training.supervised import Naive\n",
    "from avalanche.training.templates import SupervisedTemplate\n",
    "from avalanche.training.plugins import ReplayPlugin, EWCPlugin, LRSchedulerPlugin\n",
    "from avalanche.training.plugins import BiCPlugin\n",
    "\n",
    "\n",
    "# there's a bug in wandb logging of the last iteration, we have to mannually finish it\n",
    "import wandb\n",
    "import torch.optim as optim\n",
    "import warmup_scheduler\n",
    "# scenario = SplitMNIST(n_experiences=5, seed = 1234)\n",
    "\n",
    "\n",
    "\n",
    "# scenario = SplitCIFAR100(n_experiences=10, seed = 1234)\n",
    "# scenario = SplitTinyImageNet(n_experiences=10, seed = 1234)\n",
    "scenario = CORe50(mini=True)\n",
    "device = 'cuda:0'\n",
    "# MODEL CREATION\n",
    "\n",
    "# model = ViT(\n",
    "#     image_size = 28,\n",
    "#     patch_size = 7,\n",
    "#     num_classes = 10,\n",
    "#     dim = 512,\n",
    "#     depth = 3,\n",
    "#     heads = 8,\n",
    "#     mlp_dim = 256,\n",
    "#     dropout = 0.1,\n",
    "#     emb_dropout = 0.1,\n",
    "#     img_channels = 1,\n",
    "#     feature_channels = 1,\n",
    "#     dim_head = 64\n",
    "# )\n",
    "\n",
    "model = ViT(\n",
    "    image_size = 32,\n",
    "    patch_size = 4,\n",
    "    num_classes = 100,\n",
    "    dim = 512,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 384,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1,\n",
    "    img_channels = 3,\n",
    "    feature_channels = 3,\n",
    "    dim_head = 32,\n",
    "    device=device\n",
    ")\n",
    "# DEFINE THE EVALUATION PLUGIN and LOGGERS\n",
    "# The evaluation plugin manages the metrics computation.\n",
    "# It takes as argument a list of metrics, collectes their results and returns\n",
    "# them to the strategy it is attached to.\n",
    "\n",
    "loggers = []\n",
    "\n",
    "# log to Tensorboard\n",
    "# log to text file\n",
    "loggers.append(TextLogger(open('log.txt', 'a')))\n",
    "\n",
    "# print to stdout\n",
    "loggers.append(InteractiveLogger())\n",
    "\n",
    "# W&B logger - comment this if you don't have a W&B account\n",
    "loggers.append(WandBLogger(project_name=\"avalanche\", run_name=\"CIFAR100-input-nolabel\"))\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=False, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(epoch=True, epoch_running=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    confusion_matrix_metrics(num_classes=scenario.n_classes, save_image=False,\n",
    "                             stream=True),\n",
    "    loggers=loggers\n",
    ")\n",
    "\n",
    "\n",
    "# strategy plugign\n",
    "replay = ReplayPlugin(mem_size=1000)\n",
    "fsml = FewShotMetaLearningPlugin()\n",
    "ewc = EWCPlugin(ewc_lambda=1)\n",
    "bic = BiCPlugin(mem_size=1000)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3, weight_decay = 0.00005)\n",
    "# Assuming your optimizer is already defined\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 400, eta_min=1e-4)\n",
    "lr_scheduler_plugin = LRSchedulerPlugin(scheduler)\n",
    "\n",
    "# CREATE THE STRATEGY INSTANCE\n",
    "cl_strategy = SupervisedTemplate(\n",
    "    model, optimizer,\n",
    "    CrossEntropyLoss(), train_mb_size=128, train_epochs=160, eval_mb_size=128, device=device,\n",
    "    evaluator=eval_plugin,plugins=[fsml, replay, lr_scheduler_plugin], eval_every=20)\n",
    "\n",
    "\n",
    "\n",
    "# TRAINING LOOP\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "for i,experience in enumerate(scenario.train_stream[:]):\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    # train returns a dictionary which contains all the metric values\n",
    "    if i == 0:\n",
    "        cl_strategy.train_epochs = 400\n",
    "    else:\n",
    "        cl_strategy.train_epochs = 160\n",
    "\n",
    "    res = cl_strategy.train(experience, eval_streams=[scenario.test_stream[:i+1]])\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the whole test set')\n",
    "    # test also returns a dictionary which contains all the metric values\n",
    "    results.append(cl_strategy.eval(scenario.test_stream[:i+1]))\n",
    "\n",
    "# there's a bug in wandb logging of the last iteration, we have to mannually finish it\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
